
enable_pdb = False


ask_about = 'answer'


root_path = './'



class Scorer:
    def __init__(self, files, data_list_of_dicts=None):
        if not len(files):
            print('No files for evaluation')
            import sys
            sys.exit()

        from efficiency.log import fread
        data_list = []
        for file in sorted(files):
            data = fread(file)
            data_list += data
            print(file, len(data))

        import pandas as pd
        df = pd.DataFrame(data_list)
        # df = pd.read_csv(file, index_col=None)
        self.truth_pred_scorer(df)


    def apply_score_func(self, df, pred_key='pred_norm', truth_key='truth_norm'):
        if ask_about in {'graph'}:
            pred_key = 'pred'
            truth_key = 'truth'

            def score_func(row):
                def txt2edges(txt):
                    txt = txt.replace(' ', '')
                    edges = txt.split(',')
                    edges = {tuple(sorted(i.split('->', 1))) for i in edges}
                    return edges

                def edge_set2node_set(edges):
                    from efficiency.function import flatten_list
                    nodes = flatten_list(edges)
                    return set(nodes)

                from efficiency.function import get_set_f1, get_set_edit_distance

                pred_edges = txt2edges(row[pred_key])
                truth_edges = txt2edges(row[truth_key])
                edge_f1 = get_set_f1(truth_edges, pred_edges)

                pred_nodes = edge_set2node_set(pred_edges)
                truth_nodes = edge_set2node_set(truth_edges)
                node_f1 = get_set_f1(truth_nodes, pred_nodes)

                edit_distance = get_set_edit_distance(truth_edges, pred_edges)
                score_dict = {
                    'node_f1': node_f1,
                    'edge_f1': edge_f1,
                    'edge_edit_distance': edit_distance,
                    'score': edge_f1,
                }
                return score_dict
        else:
            # if ask_about in {'answer', 'query_type'}:
            score_func = lambda row: {'score': row[pred_key] == row[truth_key]}

        # df['score'] = df.apply(score_func, axis=1)
        import pandas as pd
        score_df = df.apply(lambda row: pd.Series(score_func(row)), axis=1)
        df = df.join(score_df)
        print(score_df.mean())
        score_df.describe()

        import pdb;
        pdb.set_trace()
        return df

    def truth_pred_scorer(self, df):
        #df.drop(['prompt', 'question_id'], axis=1, inplace=True)

        df = self.apply_score_func(df)
        # df['score'] = (df['pred_norm'] == df['truth_norm'])
        import pdb;
        pdb.set_trace()

        if ask_about not in {'graph'}:
            from sklearn.metrics import classification_report
            df_valid = df[~df['pred_norm'].isna()]
            for rung in [1, 2, 3]:
                print(f'Rung {rung}')
                report = classification_report(df_valid[df_valid['rung'] == rung]['truth_norm'],
                                               df_valid[df_valid['rung'] == rung]['pred_norm'], digits=4)
                print(report)
            report = classification_report(df_valid['truth_norm'], df_valid['pred_norm'], digits=4)
            print(report)

        import pdb;
        pdb.set_trace()

        res_dfs = []
        for uniq_vign_key in ['model_version']:
            try:
                res_df = self._res_by_group(df, uniq_vign_key)
                res_dfs.append(res_df)
            except:
                continue
        import pandas as pd
        res_df = pd.concat(res_dfs)
        print(res_df)
        res_df.to_csv(f'{root_path}/outputs/performance.csv')



    @staticmethod
    def _res_by_group(df, uniq_vign_key, result_key='score', return_obj=['group_dict', 'consistency_rate'][0]):
        # Group by 'group' column and count the occurrences of each value in the 'result' column
        g = df.groupby(uniq_vign_key)[result_key]
        dff = round(g.mean() * 100, 2).reset_index()
        dff['count'] = g.count().to_list()
        print(dff)
        return dff


def main():
    """
    The input file can be generated by running:
    1. generate_data_llama.py
    2. run_llama.py
    """
    llama_files=["./llama007_causal_benchmark_results.csv"]
    Scorer(llama_files)


if __name__ == '__main__':
    main()
